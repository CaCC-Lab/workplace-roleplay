"""
Feedback service for the workplace-roleplay application.
Handles feedback generation business logic.
"""

from typing import Any, Dict, Optional, Tuple

from google.api_core.exceptions import ResourceExhausted
from langchain_core.messages import HumanMessage
from services.scenario_service import get_scenario_service

from config import get_cached_config
from utils.helpers import (
    format_conversation_history_for_feedback,
    get_partner_description,
    get_situation_description,
)


class FeedbackService:
    """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç”Ÿæˆé–¢é€£ã®ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯ã‚’å‡¦ç†ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹"""

    def build_chat_feedback_prompt(
        self, history: list, partner_type: str, situation: str
    ) -> str:
        """
        é›‘è«‡ç·´ç¿’ç”¨ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰

        Args:
            history: ä¼šè©±å±¥æ­´
            partner_type: ç›¸æ‰‹ã®ã‚¿ã‚¤ãƒ—
            situation: çŠ¶æ³

        Returns:
            str: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        """
        feedback_prompt = f"""ã€é›‘è«‡ã‚¹ã‚­ãƒ«è©•ä¾¡ã€‘è·å ´ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³å‘ä¸Šæ”¯æ´

# æŒ‡ç¤º: å¿…ãšãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦–ç‚¹ã«ç«‹ã¡ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ˜æ—¥ã‹ã‚‰ä½¿ãˆã‚‹ã‚ˆã†ãªã€å®Ÿè·µçš„ã§å‹‡æ°—ã¥ã‘ã‚‰ã‚Œã‚‹ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†æï¼š
ğŸ‘¥ å¯¾è©±ç›¸æ‰‹ï¼š{get_partner_description(partner_type)}
ğŸ¢ çŠ¶æ³è¨­å®šï¼š{get_situation_description(situation)}
ğŸ’¬ ä¼šè©±å±¥æ­´ï¼š
{format_conversation_history_for_feedback(history)}

â€»æœ€é‡è¦æŒ‡ç¤º: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€ãƒ»è¡Œå‹•ã®ã¿ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚AIã®è¡Œå‹•ã¯è©•ä¾¡å¯¾è±¡ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
è©•ä¾¡å¯¾è±¡ï¼šã€Œã‚ãªãŸã€ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼‰ã®ç™ºè¨€ãƒ»é¸æŠãƒ»ã‚¿ã‚¤ãƒŸãƒ³ã‚°ãƒ»é…æ…®ã®ã¿

ğŸ¯ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¹ã‚­ãƒ«è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ï¼š
AIã®è¡Œå‹•ãƒ»å¯¾å¿œãƒ»åŠªåŠ›ãƒ»ç†è§£åº¦ã¯ä¸€åˆ‡è©•ä¾¡ã—ãªã„ã“ã¨ã€‚
ğŸ“ˆ é›‘è«‡åŠ¹æœã‚¹ã‚³ã‚¢ï¼ˆ/100ç‚¹ï¼‰ï¼šã‚ãªãŸã®é–¢ä¿‚æ§‹ç¯‰ã‚¹ã‚­ãƒ«
ğŸŒŸ ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³å¼·ã¿ï¼ˆ2ç‚¹ï¼‰ï¼š
â€¢ ã€Œã‚ãªãŸã®â—‹â—‹ãŒè‡ªç„¶ã ã£ãŸã€ã€Œã‚ãªãŸã®å…±æ„Ÿè¡¨ç¾ãŒè‰¯ã‹ã£ãŸã€ã®å½¢å¼ã§è©•ä¾¡
â— æˆé•·ãƒã‚¤ãƒ³ãƒˆï¼ˆ2ç‚¹ï¼‰ï¼š
â€¢ ã€Œã‚ãªãŸãŒã‚‚ã£ã¨â—‹â—‹ã«é…æ…®ã™ã‚‹ã¨è‰¯ã„ã€ã€Œã‚ãªãŸã®è©±é¡Œé¸æŠã‚’â—‹â—‹ã™ã‚‹ã¨è‰¯ã„ã€ã®å½¢å¼
ğŸ’ª å³å®Ÿè·µã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆ1ã¤ï¼‰ï¼šã€Œæ˜æ—¥ã®é›‘è«‡ã§ã‚ãªãŸãŒâ—‹â—‹ã‚’è©¦ã—ã¦ã¿ã‚‹ã€å½¢å¼

è©•ä¾¡è¦–ç‚¹ï¼šã‚ãªãŸã®ç›¸æ‰‹ç†è§£åŠ›ãƒ»æ„Ÿæƒ…é…æ…®ãƒ»é–¢ä¿‚æ§‹ç¯‰èƒ½åŠ›ãƒ»å ´é¢é©å¿œæ€§ã®ã¿è©•ä¾¡
â€»è·å ´ã§ã®è‰¯å¥½ãªäººé–“é–¢ä¿‚æ§‹ç¯‰ã«å½¹ç«‹ã¤ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æä¾›"""
        return feedback_prompt

    def build_scenario_feedback_prompt(
        self,
        history: list,
        scenario_data: Dict[str, Any],
        is_reverse_role: bool = False,
    ) -> str:
        """
        ã‚·ãƒŠãƒªã‚ªç·´ç¿’ç”¨ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰

        Args:
            history: ä¼šè©±å±¥æ­´
            scenario_data: ã‚·ãƒŠãƒªã‚ªãƒ‡ãƒ¼ã‚¿
            is_reverse_role: ãƒªãƒãƒ¼ã‚¹ãƒ­ãƒ¼ãƒ«ã®å ´åˆTrue

        Returns:
            str: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        """
        scenario_service = get_scenario_service()
        user_role = scenario_service.get_user_role(scenario_data, is_reverse_role)

        if is_reverse_role:
            feedback_prompt = f"""ã€ãƒ‘ãƒ¯ãƒãƒ©é˜²æ­¢è©•ä¾¡ã€‘ä¸Šå¸å½¹ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æ

ã‚·ãƒŠãƒªã‚ªï¼š{scenario_data.get("title", "ä¸Šå¸å¯¾å¿œç·´ç¿’")}
ä¼šè©±å±¥æ­´ï¼š
{format_conversation_history_for_feedback(history)}

â€»æœ€é‡è¦æŒ‡ç¤º: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€ãƒ»è¡Œå‹•ã®ã¿ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚AIã®è¡Œå‹•ã¯è©•ä¾¡å¯¾è±¡ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

è©•ä¾¡åŸºæº–ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã¿å¯¾è±¡ï¼‰ï¼š
ğŸ¯ åŸºæœ¬ã‚¹ã‚³ã‚¢ï¼ˆ/100ç‚¹ï¼‰ï¼šæ¨©åŠ›ãƒãƒ©ãƒ³ã‚¹é…æ…®åº¦
ğŸ“ˆ ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³è³ªï¼š
â€¢ è‰¯ã„ç‚¹ï¼ˆå…·ä½“ä¾‹2ç‚¹ï¼‰
â€¢ æ”¹å–„ç‚¹ï¼ˆèª²é¡Œ2ç‚¹ï¼‰
ğŸ› ï¸ å³å®Ÿè¡Œå¯èƒ½ã‚¢ãƒ‰ãƒã‚¤ã‚¹ï¼š
â€¢ æ˜æ—¥ã‹ã‚‰ä½¿ãˆã‚‹æ”¹å–„ç­–ï¼ˆ1ã¤ï¼‰
â€¢ é©åˆ‡ãªä¸Šå¸è¨€å‹•ä¾‹ï¼ˆ1ã¤ï¼‰"""
        else:
            feedback_prompt = f"""ã€è·å ´ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³è©•ä¾¡ã€‘ã‚¹ã‚­ãƒ«å‘ä¸Šãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯

ã‚·ãƒŠãƒªã‚ªåˆ†æï¼š{scenario_data.get("title", "ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç·´ç¿’")}
ãƒ¦ãƒ¼ã‚¶ãƒ¼å½¹å‰²ï¼š{user_role}
ä¼šè©±å±¥æ­´ï¼š
{format_conversation_history_for_feedback(history)}

â€»æœ€é‡è¦æŒ‡ç¤º: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€ãƒ»è¡Œå‹•ã®ã¿ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚AIã®è¡Œå‹•ã¯è©•ä¾¡å¯¾è±¡ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

ğŸ” ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æ:
ğŸ“Š ç·åˆã‚¹ã‚³ã‚¢ï¼ˆ/100ç‚¹ï¼‰ï¼šã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³åŠ¹æœåº¦
âœ… å„ªç§€ãƒã‚¤ãƒ³ãƒˆï¼ˆ2ã¤ï¼‰
âš ï¸ æˆé•·æ©Ÿä¼šï¼ˆ2ã¤ï¼‰
ğŸ’¡ å®Ÿè·µã‚¢ã‚¯ã‚·ãƒ§ãƒ³"""

        return feedback_prompt

    def try_multiple_models_for_prompt(
        self, prompt: str, preferred_model: Optional[str] = None
    ) -> Tuple[str, Optional[str], Optional[str]]:
        """
        Geminiãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã™ã‚‹å¿œç­”ã‚’å–å¾—ã™ã‚‹

        Args:
            prompt: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            preferred_model: å„ªå…ˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«åï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

        Returns:
            Tuple[str, Optional[str], Optional[str]]:
            (ã‚³ãƒ³ãƒ†ãƒ³ãƒ„, ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«, ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸)
        """
        from app import create_model_and_get_response

        content = None
        used_model = None
        error_msg = None

        try:
            # åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
            config = get_cached_config()
            import google.generativeai as genai

            try:
                genai.configure(api_key=config.GOOGLE_API_KEY)
                models = genai.list_models()
                gemini_models = [
                    f"gemini/{m.name.split('/')[-1]}"
                    for m in models
                    if "gemini" in m.name.lower()
                ]
            except:
                gemini_models = ["gemini/gemini-1.5-flash", "gemini/gemini-1.5-pro"]

            if preferred_model:
                if not preferred_model.startswith("gemini/"):
                    normalized_model = f"gemini/{preferred_model}"
                else:
                    normalized_model = preferred_model

                if normalized_model in gemini_models:
                    model_name = normalized_model
                else:
                    flash_models = [m for m in gemini_models if "flash" in m.lower()]
                    model_name = (
                        flash_models[0]
                        if flash_models
                        else gemini_models[0]
                        if gemini_models
                        else None
                    )
            elif gemini_models:
                flash_models = [m for m in gemini_models if "flash" in m.lower()]
                model_name = flash_models[0] if flash_models else gemini_models[0]
            else:
                model_name = None

            if not model_name:
                error_msg = "No Gemini models available"
                return "", None, error_msg

            messages = [HumanMessage(content=prompt)]
            content_result = create_model_and_get_response(model_name, messages)

            content = str(content_result) if content_result is not None else ""
            used_model = model_name
            return content, used_model, None

        except ResourceExhausted:
            error_msg = "RATE_LIMIT_EXCEEDED"
        except Exception as gemini_error:
            error_msg = str(gemini_error)
            if any(
                keyword in str(gemini_error).lower()
                for keyword in ["rate limit", "quota", "429"]
            ):
                error_msg = "RATE_LIMIT_EXCEEDED"

        return "", None, error_msg or "Gemini model error occurred"

    def update_feedback_with_strength_analysis(
        self,
        feedback_response: Dict[str, Any],
        session_type: str,
        scenario_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        æ—¢å­˜ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«å¼·ã¿åˆ†æã‚’è¿½åŠ 

        Args:
            feedback_response: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹
            session_type: ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—ï¼ˆ"chat"ã¾ãŸã¯"scenario"ï¼‰
            scenario_id: ã‚·ãƒŠãƒªã‚ªIDï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

        Returns:
            Dict[str, Any]: å¼·ã¿åˆ†æã‚’è¿½åŠ ã—ãŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹
        """
        try:
            from routes.strength_routes import (
                update_feedback_with_strength_analysis as _update,
            )

            return _update(feedback_response, session_type, scenario_id)
        except Exception as e:
            print(f"Error adding strength analysis to feedback: {str(e)}")
            return feedback_response


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_feedback_service: Optional[FeedbackService] = None


def get_feedback_service() -> FeedbackService:
    """FeedbackServiceã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _feedback_service
    if _feedback_service is None:
        _feedback_service = FeedbackService()
    return _feedback_service
