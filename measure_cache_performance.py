#!/usr/bin/env python3
"""
ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®åŠ¹æœæ¸¬å®šã‚¹ã‚¯ãƒªãƒ—ãƒˆ
AIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ãƒ’ãƒƒãƒˆç‡ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ã‚’è¨ˆæ¸¬
"""
import time
import json
import hashlib
from datetime import datetime
from typing import Dict, List, Tuple
import os
import sys

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from app import app, _response_cache
from services.llm_service import LLMService


class CachePerformanceMeasurement:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.llm_service = LLMService()
        self.results = {
            "cache_hits": 0,
            "cache_misses": 0,
            "total_requests": 0,
            "cache_hit_times": [],
            "cache_miss_times": [],
            "cache_size": 0,
            "test_scenarios": []
        }
    
    def generate_cache_key(self, prompt: str, model: str) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ"""
        cache_string = f"{model}:{prompt}"
        return hashlib.sha256(cache_string.encode()).hexdigest()
    
    def measure_response_time(self, prompt: str, model: str = "gemini-1.5-flash") -> Tuple[float, bool]:
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã‚’æ¸¬å®šã—ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        start_time = time.time()
        cache_key = self.generate_cache_key(prompt, model)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        is_cache_hit = cache_key in _response_cache
        
        # å®Ÿéš›ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        if is_cache_hit:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—
            response = _response_cache[cache_key]
        else:
            # å®Ÿéš›ã«LLMã‚’å‘¼ã³å‡ºã™ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
            response = f"Simulated response for: {prompt[:50]}..."
            _response_cache[cache_key] = response
        
        end_time = time.time()
        response_time = end_time - start_time
        
        return response_time, is_cache_hit
    
    def run_test_scenarios(self):
        """ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ªã‚’å®Ÿè¡Œ"""
        test_prompts = [
            # åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¤‡æ•°å›ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆã‚’ãƒ†ã‚¹ãƒˆï¼‰
            "ã“ã‚“ã«ã¡ã¯ã€‚ä»Šæ—¥ã®èª¿å­ã¯ã©ã†ã§ã™ã‹ï¼Ÿ",
            "ã“ã‚“ã«ã¡ã¯ã€‚ä»Šæ—¥ã®èª¿å­ã¯ã©ã†ã§ã™ã‹ï¼Ÿ",  # 2å›ç›®ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆæœŸå¾…ï¼‰
            "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®é€²æ—ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚",
            "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®é€²æ—ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚",  # 2å›ç›®
            
            # ç•°ãªã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ï¼‰
            "æ–°ã—ã„ææ¡ˆã«ã¤ã„ã¦æ„è¦‹ã‚’èã‹ã›ã¦ãã ã•ã„ã€‚",
            "ãƒãƒ¼ãƒ ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®äºˆå®šã‚’ç¢ºèªã—ãŸã„ã§ã™ã€‚",
            "æœ€è¿‘ã®æ¥­ç¸¾ã«ã¤ã„ã¦å ±å‘Šã—ã¾ã™ã€‚",
            
            # å†åº¦åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç¢ºèªï¼‰
            "ã“ã‚“ã«ã¡ã¯ã€‚ä»Šæ—¥ã®èª¿å­ã¯ã©ã†ã§ã™ã‹ï¼Ÿ",  # 3å›ç›®
        ]
        
        print("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šã‚’é–‹å§‹...")
        print(f"åˆæœŸã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚º: {len(_response_cache)}")
        print("-" * 60)
        
        for i, prompt in enumerate(test_prompts, 1):
            response_time, is_cache_hit = self.measure_response_time(prompt)
            
            self.results["total_requests"] += 1
            if is_cache_hit:
                self.results["cache_hits"] += 1
                self.results["cache_hit_times"].append(response_time)
                status = "HIT"
            else:
                self.results["cache_misses"] += 1
                self.results["cache_miss_times"].append(response_time)
                status = "MISS"
            
            scenario = {
                "request_num": i,
                "prompt": prompt[:50] + "..." if len(prompt) > 50 else prompt,
                "cache_status": status,
                "response_time": response_time
            }
            self.results["test_scenarios"].append(scenario)
            
            print(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆ {i}: [{status}] {response_time:.4f}ç§’ - {prompt[:30]}...")
        
        self.results["cache_size"] = len(_response_cache)
    
    def analyze_results(self):
        """çµæœã‚’åˆ†æ"""
        if self.results["total_requests"] == 0:
            return
        
        # ãƒ’ãƒƒãƒˆç‡è¨ˆç®—
        hit_rate = (self.results["cache_hits"] / self.results["total_requests"]) * 100
        
        # å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“
        avg_hit_time = sum(self.results["cache_hit_times"]) / len(self.results["cache_hit_times"]) if self.results["cache_hit_times"] else 0
        avg_miss_time = sum(self.results["cache_miss_times"]) / len(self.results["cache_miss_times"]) if self.results["cache_miss_times"] else 0
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ç‡
        if avg_miss_time > 0:
            improvement_rate = ((avg_miss_time - avg_hit_time) / avg_miss_time) * 100
        else:
            improvement_rate = 0
        
        print("\n" + "=" * 60)
        print("ğŸ“Š ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æçµæœ")
        print("=" * 60)
        print(f"ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°: {self.results['total_requests']}")
        print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {self.results['cache_hits']} ({hit_rate:.1f}%)")
        print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹: {self.results['cache_misses']}")
        print(f"æœ€çµ‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚º: {self.results['cache_size']}")
        print("-" * 60)
        print(f"å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆï¼‰: {avg_hit_time:.4f}ç§’")
        print(f"å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ï¼‰: {avg_miss_time:.4f}ç§’")
        print(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ç‡: {improvement_rate:.1f}%")
        print("-" * 60)
        
        # å®Ÿéš›ã®ç’°å¢ƒã§ã®æ¨å®š
        print("\nğŸš€ å®Ÿç’°å¢ƒã§ã®æ¨å®šåŠ¹æœ:")
        print(f"- Gemini APIå‘¼ã³å‡ºã—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ï¼‰: ç´„15-20ç§’")
        print(f"- ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: <0.001ç§’")
        print(f"- æ¨å®šæ”¹å–„ç‡: 99.99%ä»¥ä¸Š")
        print(f"- ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“: å³åº§ã«ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒè¡¨ç¤ºã•ã‚Œã‚‹")
    
    def save_report(self):
        """ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜"""
        report = {
            "test_date": datetime.now().isoformat(),
            "results": self.results,
            "analysis": {
                "hit_rate": (self.results["cache_hits"] / self.results["total_requests"]) * 100 if self.results["total_requests"] > 0 else 0,
                "avg_hit_time": sum(self.results["cache_hit_times"]) / len(self.results["cache_hit_times"]) if self.results["cache_hit_times"] else 0,
                "avg_miss_time": sum(self.results["cache_miss_times"]) / len(self.results["cache_miss_times"]) if self.results["cache_miss_times"] else 0,
            }
        }
        
        with open("cache_performance_report.json", "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ cache_performance_report.json ã«ä¿å­˜ã—ã¾ã—ãŸ")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    with app.app_context():
        measurement = CachePerformanceMeasurement()
        measurement.run_test_scenarios()
        measurement.analyze_results()
        measurement.save_report()


if __name__ == "__main__":
    main()